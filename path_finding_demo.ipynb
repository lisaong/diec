{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "path_finding_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIgT6RAukXzhtdUoiROvm+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/diec/blob/rl_path_finding/path_finding_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsIdZTi60PM",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning Path-Finding Demo\n",
        "\n",
        "This demonstrates how to:\n",
        "- Use OpenAI gym to create a custom environment\n",
        "- Compare different Q-learning algorithms for Reinforcement Learning\n",
        "\n",
        "Inspired by: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
        "\n",
        "## Problem Setup\n",
        "\n",
        "Bender is lost in Fry's house! Help Bender find Fry (who is in Room 5 waiting with a can of beer).\n",
        "\n",
        "![intro](https://github.com/lisaong/diec/raw/rl_path_finding/day4/rl/path_finding_intro.png)\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) is an open-source Python toolkit for developing RL algorithms.\n",
        "\n",
        "We will use OpenAI gym to re-create Fry's house, then run some reinforcement learning to find the path.\n",
        "\n",
        "Fry's house will be a custom environment, similar to: https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvDQ9PwW6SFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2c8721e-1650-465a-ffe4-345ab8731835"
      },
      "source": [
        "# gym is already built into Colab\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "gym.__version__"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBlQwqV_dSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FrysHomeEnv(gym.Env):\n",
        "  \"\"\"Custom Environment describing Fry's home  \n",
        "  \n",
        "  For details on the gym.Env class:\n",
        "  https://github.com/openai/gym/blob/master/gym/core.py\n",
        "  \"\"\"\n",
        "\n",
        "  # render to the current display or terminal\n",
        "  metadata = {'render.modes': ['human']}\n",
        "\n",
        "  def __init__(self):\n",
        "    super(FrysHomeEnv, self).__init__()\n",
        "\n",
        "    # Initialise the rewards matrix according to the graph above\n",
        "    # Where:\n",
        "    #  state: current room, action: next room\n",
        "    #  dimensions (row=state, col=actions)\n",
        "    #  A value of -1 means there is no adjacent path from room_i to room_j\n",
        "    #  (for example, room_0 to room_0 has, room_0 to room_5)\n",
        "    self.rewards = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "                             [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "                             [-1, -1, -1,  0, -1, -1], # etc\n",
        "                             [-1,  0,  0, -1,  0, -1],\n",
        "                             [ 0, -1, -1,  0, -1,  0],\n",
        "                             [-1, 100, -1, -1, 100, 100]])\n",
        "    \n",
        "    self.num_rooms = self.rewards.shape[0]\n",
        "\n",
        "    # Action space describes all possible actions that can be taken\n",
        "    # here, we can select 1 out of 6 rooms\n",
        "    self.action_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Observation space describes the valid observations\n",
        "    # since we are moving between rooms, we can be in 1 of 6 rooms\n",
        "    self.observation_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Rewards range describes the min and max possible rewards\n",
        "    self.reward_range = (self.rewards.min(), self.rewards.max())\n",
        "\n",
        "    # Room 5 is our goal\n",
        "    self.goal = 5\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to an initial state\"\"\"\n",
        "\n",
        "    # Randomly initialise the state\n",
        "    self.state = random.randint(0, self.num_rooms)\n",
        "\n",
        "    # Return the observation (same as the state in our case)\n",
        "    obs = self.state\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one step within the environment\"\"\"\n",
        "\n",
        "    # take the selected action\n",
        "    prev_state = self.state\n",
        "    self.state = action\n",
        "\n",
        "    # calculate the reward\n",
        "    reward = self.rewards[prev_state][action]\n",
        "\n",
        "    # check if we've reached our goal\n",
        "    done = (self.state == self.goal)\n",
        "\n",
        "    # get the next observation\n",
        "    obs = self.state\n",
        "\n",
        "    return obs, reward, done, {}\n",
        "\n",
        "  def render(self, mode='human', close=True):\n",
        "    print(f'Current room: {self.state}')\n",
        "    print(f'Reached goal: {self.state == self.goal}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyjONcEzEzTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ufa52UWRGLG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}