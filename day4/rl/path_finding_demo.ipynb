{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "path_finding_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPW54OuMfuS/kH7+EZjzout",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/diec/blob/rl_path_finding/day4/rl/path_finding_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsIdZTi60PM",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning Path-Finding Demo\n",
        "\n",
        "This demonstrates how to:\n",
        "- Use OpenAI gym to create a custom environment\n",
        "- Compare different Q-learning algorithms for Reinforcement Learning\n",
        "\n",
        "Inspired by: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
        "\n",
        "## Problem Setup\n",
        "\n",
        "Bender is lost in Fry's house! Help Bender find Fry (who is in Room 5 waiting with a can of beer).\n",
        "\n",
        "![intro](https://github.com/lisaong/diec/raw/rl_path_finding/day4/rl/path_finding_intro.png)\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) is an open-source Python toolkit for developing RL algorithms.\n",
        "\n",
        "We will use OpenAI gym to re-create Fry's house, then run some reinforcement learning to find the path.\n",
        "\n",
        "https://github.com/openai/gym/blob/master/docs/creating-environments.md\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvDQ9PwW6SFZ",
        "colab_type": "code",
        "outputId": "f87f8df8-d0eb-4c42-d516-9ff3a2fb9d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gym is already built into Colab\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "gym.__version__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBlQwqV_dSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# A Simple Path Finding OpenAI Gym Environment\n",
        "#\n",
        "# Inspired by: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
        "#\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class FrysHomeEnv(gym.Env):\n",
        "  \"\"\"Custom Environment describing Fry's home  \n",
        "  \n",
        "  For details on the gym.Env class:\n",
        "  https://github.com/openai/gym/blob/master/gym/core.py\n",
        "  \"\"\"\n",
        "\n",
        "  # render to the current display or terminal\n",
        "  metadata = {'render.modes': ['human']}\n",
        "\n",
        "  def __init__(self, rewards):\n",
        "    super(FrysHomeEnv, self).__init__()\n",
        "\n",
        "    self.rewards = rewards\n",
        "    self.num_rooms = self.rewards.shape[0]\n",
        "\n",
        "    # Action space describes all possible actions that can be taken\n",
        "    # here, we can select 1 out of 6 rooms\n",
        "    self.action_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Observation space describes the valid observations\n",
        "    # since we are moving between rooms, we can be in 1 of 6 rooms\n",
        "    self.observation_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Rewards range describes the min and max possible rewards\n",
        "    self.reward_range = (self.rewards.min(), self.rewards.max())\n",
        "\n",
        "    # Room 5 is our goal\n",
        "    self.goal = 5\n",
        "\n",
        "    # Initialise our state\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to an initial state\"\"\"\n",
        "\n",
        "    # Randomly initialise the state\n",
        "    self.state = random.randint(0, self.num_rooms-1)\n",
        "\n",
        "    # Return the observation (same as the state in our case)\n",
        "    obs = self.state\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one step within the environment\"\"\"\n",
        "\n",
        "    # take the selected action\n",
        "    prev_state = self.state\n",
        "    self.state = action\n",
        "\n",
        "    # calculate the reward\n",
        "    reward = self.rewards[prev_state][action]\n",
        "\n",
        "    # check if we've reached our goal\n",
        "    done = (prev_state == self.goal or self.state == self.goal)\n",
        "\n",
        "    # get the next observation\n",
        "    obs = self.state\n",
        "\n",
        "    return obs, reward, done, {}\n",
        "\n",
        "  def render(self, mode='human', close=True):\n",
        "    \"\"\"Print state of the current environment\"\"\"\n",
        "    print(f'Current room: {self.state}, Reached goal: {self.state == self.goal}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyjONcEzEzTj",
        "colab_type": "code",
        "outputId": "e93380b5-d60a-4581-b91a-49634e62eb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Unit testing\n",
        "\n",
        "# Initialise the rewards matrix for the rooms in the house\n",
        "# Where:\n",
        "#  state: current room, action: next room\n",
        "#  dimensions (row=state, col=actions)\n",
        "#  A value of -1 means there is no adjacent path from room_i to room_j\n",
        "#  (for example, room_0 to room_0 has, room_0 to room_5)\n",
        "R = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "              [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "              [-1, -1, -1,  0, -1, -1], # etc\n",
        "              [-1,  0,  0, -1,  0, -1],\n",
        "              [ 0, -1, -1,  0, -1,  0],\n",
        "              [-1, 100, -1, -1, 100, 100]])\n",
        "\n",
        "myenv = FrysHomeEnv(rewards=R)\n",
        "\n",
        "for i in range(0, 6):\n",
        "  print(myenv.step(i))\n",
        "  myenv.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, -1, True, {})\n",
            "Current room: 0, Reached goal: False\n",
            "(1, -1, False, {})\n",
            "Current room: 1, Reached goal: False\n",
            "(2, -1, False, {})\n",
            "Current room: 2, Reached goal: False\n",
            "(3, 0, False, {})\n",
            "Current room: 3, Reached goal: False\n",
            "(4, 0, False, {})\n",
            "Current room: 4, Reached goal: False\n",
            "(5, 0, True, {})\n",
            "Current room: 5, Reached goal: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ8RA_ilKtkX",
        "colab_type": "text"
      },
      "source": [
        "## Package custom environment as module\n",
        "\n",
        "OpenAI gym requires all environments to be packaged as Python modules.\n",
        "\n",
        "The code above has been packaged here:\n",
        "https://github.com/lisaong/diec/blob/master/day4/rl/gym-fryshome\n",
        "\n",
        "The module follows this convention:\n",
        "https://github.com/openai/gym/blob/master/docs/creating-environments.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBhgYzDkN46S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ded5eeee-80c1-4346-8b85-059aa457379e"
      },
      "source": [
        "!git clone -b rl_path_finding https://github.com/lisaong/diec.git\n",
        "%cd diec/day4/rl/gym-fryshome\n",
        "!git pull\n",
        "!pip install --verbose -e  ."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'diec' already exists and is not an empty directory.\n",
            "/content/diec/day4/rl/gym-fryshome\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 17 (delta 11), reused 12 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "From https://github.com/lisaong/diec\n",
            "   f52e36d..b8d2558  rl_path_finding -> origin/rl_path_finding\n",
            "Updating f52e36d..b8d2558\n",
            "Fast-forward\n",
            " .../gym_fryshome/envs/frys_home_env.py             |  16 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " day4/rl/path_finding_demo.ipynb                    | 254 \u001b[32m++++++++++++\u001b[m\u001b[31m---------\u001b[m\n",
            " 2 files changed, 144 insertions(+), 126 deletions(-)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-39sk297y\n",
            "Created temporary directory: /tmp/pip-req-tracker-w83an5vv\n",
            "Created requirements tracker '/tmp/pip-req-tracker-w83an5vv'\n",
            "Created temporary directory: /tmp/pip-install-883a0bjx\n",
            "Obtaining file:///content/diec/day4/rl/gym-fryshome\n",
            "  Added file:///content/diec/day4/rl/gym-fryshome to build tracker '/tmp/pip-req-tracker-w83an5vv'\n",
            "    Running setup.py (path:/content/diec/day4/rl/gym-fryshome/setup.py) egg_info for package from file:///content/diec/day4/rl/gym-fryshome\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    writing gym_fryshome.egg-info/PKG-INFO\n",
            "    writing dependency_links to gym_fryshome.egg-info/dependency_links.txt\n",
            "    writing requirements to gym_fryshome.egg-info/requires.txt\n",
            "    writing top-level names to gym_fryshome.egg-info/top_level.txt\n",
            "    writing manifest file 'gym_fryshome.egg-info/SOURCES.txt'\n",
            "  Source in /content/diec/day4/rl/gym-fryshome has version 0.0.1, which satisfies requirement gym-fryshome==0.0.1 from file:///content/diec/day4/rl/gym-fryshome\n",
            "  Removed gym-fryshome==0.0.1 from file:///content/diec/day4/rl/gym-fryshome from build tracker '/tmp/pip-req-tracker-w83an5vv'\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-fryshome==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym-fryshome==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-fryshome==0.0.1) (0.16.0)\n",
            "Installing collected packages: gym-fryshome\n",
            "  Found existing installation: gym-fryshome 0.0.1\n",
            "    Not sure how to uninstall: gym-fryshome 0.0.1 - Check: /content/diec/day4/rl/gym-fryshome\n",
            "    Can't uninstall 'gym-fryshome'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-fryshome\n",
            "    Running command /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/content/diec/day4/rl/gym-fryshome/setup.py'\"'\"'; __file__='\"'\"'/content/diec/day4/rl/gym-fryshome/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\n",
            "    running develop\n",
            "    running egg_info\n",
            "    writing gym_fryshome.egg-info/PKG-INFO\n",
            "    writing dependency_links to gym_fryshome.egg-info/dependency_links.txt\n",
            "    writing requirements to gym_fryshome.egg-info/requires.txt\n",
            "    writing top-level names to gym_fryshome.egg-info/top_level.txt\n",
            "    writing manifest file 'gym_fryshome.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    Creating /usr/local/lib/python3.6/dist-packages/gym-fryshome.egg-link (link to .)\n",
            "    gym-fryshome 0.0.1 is already the active version in easy-install.pth\n",
            "\n",
            "    Installed /content/diec/day4/rl/gym-fryshome\n",
            "Successfully installed gym-fryshome\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-w83an5vv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ITTKEVV_O8",
        "colab_type": "text"
      },
      "source": [
        "## ** Restart the Colab kernel after every pip install**\n",
        "\n",
        "Runtime -> Restart Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zj5DFniQ11y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d31f79c5-c7d1-46e8-df36-80827bb79a4a"
      },
      "source": [
        "# IMPORTANT: RESTART THE COLAB KERNEL if you've just run !pip install\n",
        "\n",
        "# Test the installation\n",
        "import gym_fryshome\n",
        "\n",
        "gym_fryshome"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'gym_fryshome' from '/content/diec/day4/rl/gym-fryshome/gym_fryshome/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2iBf7ZeJgeT",
        "colab_type": "text"
      },
      "source": [
        "## Random Walk Agent\n",
        " \n",
        "The simplest way is to have Bender randomly walk around the house.\n",
        "* We will run 20 episodes, where each episode is a maximum of 100 steps\n",
        "* Refer to http://gym.openai.com/docs/\n",
        "\n",
        "This doesn't actually learn anything, but is a good baseline for any RL agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpFOINPf9Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomAgent:\n",
        "    \"\"\"The world's simplest agent!\n",
        "      https://github.com/openai/gym/blob/master/examples/agents/random_agent.py\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        return self.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycXafXEJ-tR",
        "colab_type": "code",
        "outputId": "485e3a71-75f5-4da7-fb3c-3e0d784ba0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "EPISODE_COUNT = 20\n",
        "STEPS_PER_EPISODE = 100\n",
        "R = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "              [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "              [-1, -1, -1,  0, -1, -1], # etc\n",
        "              [-1,  0,  0, -1,  0, -1],\n",
        "              [ 0, -1, -1,  0, -1,  0],\n",
        "              [-1, 100, -1, -1, 100, 100]])\n",
        "\n",
        "# Global state\n",
        "done = False\n",
        "reward = 0\n",
        "\n",
        "# Create our environment (Fry's home), and our agent\n",
        "env = gym.make('gym_fryshome:fryshome-v0', rewards=R)\n",
        "bender_agent = RandomAgent(env.action_space)\n",
        "\n",
        "for episode in range(EPISODE_COUNT):\n",
        "  observation = env.reset()\n",
        "\n",
        "  for t in range(STEPS_PER_EPISODE):\n",
        "    env.render()\n",
        "    \n",
        "    # take a random action\n",
        "    action = bender_agent.act(observation, reward, done)\n",
        "\n",
        "    # step the environment using the selected action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      print(f'Episode finished after {t+1} timesteps\\n')\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 9 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 13 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 8 timesteps\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blqh8_sZbXEW",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning Agent\n",
        "\n",
        "Let's implement the basic Q-Learning formula (without Temporal Differencing):\n",
        "\n",
        "`Q(state, action) = R(state, action) + gamma * max[Q(next_state, all actions)]`\n",
        "\n",
        "Our Q-Learning agent will store the Q-values as we go along. This can be thought of as Benders \"brain\"!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5D-vv4_cWox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "class QLearningAgent:\n",
        "  \"\"\"Basic Q-Learning Agent\"\"\"\n",
        "  def __init__(self, rewards, gamma=0.8):\n",
        "    \"\"\"rewards: the rewards matrix\n",
        "    gamma: the discount factor in considering future rewards\n",
        "    \"\"\"\n",
        "    self.rewards = rewards\n",
        "\n",
        "    self.action_space = spaces.Discrete(rewards.shape[0])\n",
        "    self.action = self.action_space.sample()\n",
        "\n",
        "    self.actions = np.arange(rewards.shape[0])\n",
        "\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # Initialise the Q-matrix to zeros:\n",
        "    # dimensions (row=state, cols=actions)\n",
        "    self.Q = np.zeros(rewards.shape)\n",
        "\n",
        "  def _get_valid_actions(self, observation):\n",
        "    return self.actions[self.rewards[observation] != -1]\n",
        "\n",
        "  def act(self, observation, reward, done):\n",
        "    \"\"\"Update the Q-matrix, then take an action\n",
        "    observation: current state\n",
        "    reward: reward from the previous action\n",
        "    done: whether the episode is completed\n",
        "    \"\"\"\n",
        "    if done:\n",
        "      return self.action # no change, we are done\n",
        "\n",
        "    # randomly select the next action (and next observation)\n",
        "    valid_actions = self._get_valid_actions(observation)\n",
        "    self.action = np.random.choice(valid_actions)\n",
        "    next_observation = self.action\n",
        "\n",
        "    # find the maximum reward for all actions given the next observation\n",
        "    all_actions = self._get_valid_actions(next_observation)\n",
        "    future_rewards = self.rewards[next_observation][all_actions]\n",
        "\n",
        "    print(f'Next action/observation: {next_observation}, all actions: \\\n",
        "{all_actions}, all future rewards: {future_rewards}, max future reward: \\\n",
        "{future_rewards.max()}')\n",
        "\n",
        "    # update the Q matrix\n",
        "    self.Q[observation][self.action] = self.rewards[observation][self.action] + \\\n",
        "      self.gamma * future_rewards.max()\n",
        "    print(f'Q-values: {self.Q}')\n",
        "\n",
        "    return self.action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_aCvSQRd1WA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5e5c5c5d-61ea-4365-800b-d4a7da002192"
      },
      "source": [
        "# Unit Test\n",
        "R = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "              [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "              [-1, -1, -1,  0, -1, -1], # etc\n",
        "              [-1,  0,  0, -1,  0, -1],\n",
        "              [ 0, -1, -1,  0, -1,  0],\n",
        "              [-1, 100, -1, -1, 100, 100]])\n",
        "\n",
        "bender_agent_ql = QLearningAgent(rewards=R)\n",
        "bender_agent_ql.act(1, 0, False)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "[[ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. 80.]\n",
            " [ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO2ftSOKoASQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "84a15e05-b298-4a73-e046-77a436a74a43"
      },
      "source": [
        "spaces.Discrete(R.shape[0])[False, False, False,  True, False,  True]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5b8826f9db86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscrete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'Discrete' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSFnPsyjohbc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f8d5bd2-9589-464e-dedc-dd0300a8804a"
      },
      "source": [
        "np.arange(5)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkaJhf12dham",
        "colab_type": "text"
      },
      "source": [
        "## More Advanced Agents\n",
        "\n",
        "You can install baselines, which contain implementation of more sophisticated agents.\n",
        "\n",
        "https://github.com/openai/baselines"
      ]
    }
  ]
}