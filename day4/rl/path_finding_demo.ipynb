{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "path_finding_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP7/jjpL0ffufIJ6XS/ifQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/diec/blob/rl_path_finding/day4/rl/path_finding_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsIdZTi60PM",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning Path-Finding Demo\n",
        "\n",
        "This demonstrates how to:\n",
        "- Use OpenAI gym to create a custom environment\n",
        "- Compare different Q-learning algorithms for Reinforcement Learning\n",
        "\n",
        "Inspired by: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
        "\n",
        "## Problem Setup\n",
        "\n",
        "Bender is lost in Fry's house! Help Bender find Fry (who is in Room 5 waiting with a can of beer).\n",
        "\n",
        "![intro](https://github.com/lisaong/diec/raw/rl_path_finding/day4/rl/path_finding_intro.png)\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) is an open-source Python toolkit for developing RL algorithms.\n",
        "\n",
        "We will use OpenAI gym to re-create Fry's house, then run some reinforcement learning to find the path.\n",
        "\n",
        "https://github.com/openai/gym/blob/master/docs/creating-environments.md\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvDQ9PwW6SFZ",
        "colab_type": "code",
        "outputId": "f87f8df8-d0eb-4c42-d516-9ff3a2fb9d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gym is already built into Colab\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "gym.__version__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBlQwqV_dSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FrysHomeEnv(gym.Env):\n",
        "  \"\"\"Custom Environment describing Fry's home  \n",
        "  \n",
        "  For details on the gym.Env class:\n",
        "  https://github.com/openai/gym/blob/master/gym/core.py\n",
        "  \"\"\"\n",
        "\n",
        "  # render to the current display or terminal\n",
        "  metadata = {'render.modes': ['human']}\n",
        "\n",
        "  def __init__(self):\n",
        "    super(FrysHomeEnv, self).__init__()\n",
        "\n",
        "    # Initialise the rewards matrix according to the graph above\n",
        "    # Where:\n",
        "    #  state: current room, action: next room\n",
        "    #  dimensions (row=state, col=actions)\n",
        "    #  A value of -1 means there is no adjacent path from room_i to room_j\n",
        "    #  (for example, room_0 to room_0 has, room_0 to room_5)\n",
        "    self.rewards = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "                             [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "                             [-1, -1, -1,  0, -1, -1], # etc\n",
        "                             [-1,  0,  0, -1,  0, -1],\n",
        "                             [ 0, -1, -1,  0, -1,  0],\n",
        "                             [-1, 100, -1, -1, 100, 100]])\n",
        "    \n",
        "    self.num_rooms = self.rewards.shape[0]\n",
        "\n",
        "    # Action space describes all possible actions that can be taken\n",
        "    # here, we can select 1 out of 6 rooms\n",
        "    self.action_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Observation space describes the valid observations\n",
        "    # since we are moving between rooms, we can be in 1 of 6 rooms\n",
        "    self.observation_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Rewards range describes the min and max possible rewards\n",
        "    self.reward_range = (self.rewards.min(), self.rewards.max())\n",
        "\n",
        "    # Room 5 is our goal\n",
        "    self.goal = 5\n",
        "\n",
        "    # Initialise our state\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to an initial state\"\"\"\n",
        "\n",
        "    # Randomly initialise the state\n",
        "    self.state = random.randint(0, self.num_rooms)\n",
        "\n",
        "    # Return the observation (same as the state in our case)\n",
        "    obs = self.state\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one step within the environment\"\"\"\n",
        "\n",
        "    # take the selected action\n",
        "    prev_state = self.state\n",
        "    self.state = action\n",
        "\n",
        "    # calculate the reward\n",
        "    reward = self.rewards[prev_state][action]\n",
        "\n",
        "    # check if we've reached our goal\n",
        "    done = (self.state == self.goal)\n",
        "\n",
        "    # get the next observation\n",
        "    obs = self.state\n",
        "\n",
        "    return obs, reward, done, {}\n",
        "\n",
        "  def render(self, mode='human', close=True):\n",
        "    print(f'Current room: {self.state}')\n",
        "    print(f'Reached goal: {self.state == self.goal}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyjONcEzEzTj",
        "colab_type": "code",
        "outputId": "2cafde1a-4ec7-4069-93eb-f0ff58f9cf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Unit testing\n",
        "myenv = FrysHomeEnv()\n",
        "\n",
        "for i in range(0, 6):\n",
        "  print(myenv.step(i))\n",
        "  myenv.render()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, -1, False, {})\n",
            "Current room: 0\n",
            "Reached goal: False\n",
            "(1, -1, False, {})\n",
            "Current room: 1\n",
            "Reached goal: False\n",
            "(2, -1, False, {})\n",
            "Current room: 2\n",
            "Reached goal: False\n",
            "(3, 0, False, {})\n",
            "Current room: 3\n",
            "Reached goal: False\n",
            "(4, 0, False, {})\n",
            "Current room: 4\n",
            "Reached goal: False\n",
            "(5, 0, True, {})\n",
            "Current room: 5\n",
            "Reached goal: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ8RA_ilKtkX",
        "colab_type": "text"
      },
      "source": [
        "## Package custom environment as module\n",
        "\n",
        "OpenAI gym requires all environments to be packaged as Python modules.\n",
        "\n",
        "The code above has been packaged here:\n",
        "https://github.com/lisaong/diec/blob/master/day4/rl/gym-fryshome\n",
        "\n",
        "The module follows this convention:\n",
        "https://github.com/openai/gym/blob/master/docs/creating-environments.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBhgYzDkN46S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50ff7a4f-10fb-4e22-baf4-1bb927d98b23"
      },
      "source": [
        "!git clone -b rl_path_finding https://github.com/lisaong/diec.git\n",
        "%cd diec/day4/rl/gym-fryshome\n",
        "!git pull\n",
        "!pip install --verbose -e  ."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'diec' already exists and is not an empty directory.\n",
            "/content/diec/day4/rl/gym-fryshome\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 8 (delta 4), reused 8 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (8/8), done.\n",
            "From https://github.com/lisaong/diec\n",
            "   25cf3b7..51a04be  rl_path_finding -> origin/rl_path_finding\n",
            "Updating 25cf3b7..51a04be\n",
            "Fast-forward\n",
            " day4/rl/gym-fryshome/gym_fryshome/envs/frys_home_env.py | 3 \u001b[32m+\u001b[m\u001b[31m--\u001b[m\n",
            " 1 file changed, 1 insertion(+), 2 deletions(-)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-vm4bt16c\n",
            "Created temporary directory: /tmp/pip-req-tracker-2eyibxib\n",
            "Created requirements tracker '/tmp/pip-req-tracker-2eyibxib'\n",
            "Created temporary directory: /tmp/pip-install-6tiunkpb\n",
            "Obtaining file:///content/diec/day4/rl/gym-fryshome\n",
            "  Added file:///content/diec/day4/rl/gym-fryshome to build tracker '/tmp/pip-req-tracker-2eyibxib'\n",
            "    Running setup.py (path:/content/diec/day4/rl/gym-fryshome/setup.py) egg_info for package from file:///content/diec/day4/rl/gym-fryshome\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    writing gym_fryshome.egg-info/PKG-INFO\n",
            "    writing dependency_links to gym_fryshome.egg-info/dependency_links.txt\n",
            "    writing requirements to gym_fryshome.egg-info/requires.txt\n",
            "    writing top-level names to gym_fryshome.egg-info/top_level.txt\n",
            "    writing manifest file 'gym_fryshome.egg-info/SOURCES.txt'\n",
            "  Source in /content/diec/day4/rl/gym-fryshome has version 0.0.1, which satisfies requirement gym-fryshome==0.0.1 from file:///content/diec/day4/rl/gym-fryshome\n",
            "  Removed gym-fryshome==0.0.1 from file:///content/diec/day4/rl/gym-fryshome from build tracker '/tmp/pip-req-tracker-2eyibxib'\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-fryshome==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym-fryshome==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-fryshome==0.0.1) (0.16.0)\n",
            "Installing collected packages: gym-fryshome\n",
            "  Found existing installation: gym-fryshome 0.0.1\n",
            "    Not sure how to uninstall: gym-fryshome 0.0.1 - Check: /content/diec/day4/rl/gym-fryshome\n",
            "    Can't uninstall 'gym-fryshome'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-fryshome\n",
            "    Running command /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/content/diec/day4/rl/gym-fryshome/setup.py'\"'\"'; __file__='\"'\"'/content/diec/day4/rl/gym-fryshome/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\n",
            "    running develop\n",
            "    running egg_info\n",
            "    writing gym_fryshome.egg-info/PKG-INFO\n",
            "    writing dependency_links to gym_fryshome.egg-info/dependency_links.txt\n",
            "    writing requirements to gym_fryshome.egg-info/requires.txt\n",
            "    writing top-level names to gym_fryshome.egg-info/top_level.txt\n",
            "    writing manifest file 'gym_fryshome.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    Creating /usr/local/lib/python3.6/dist-packages/gym-fryshome.egg-link (link to .)\n",
            "    gym-fryshome 0.0.1 is already the active version in easy-install.pth\n",
            "\n",
            "    Installed /content/diec/day4/rl/gym-fryshome\n",
            "Successfully installed gym-fryshome\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-2eyibxib'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ITTKEVV_O8",
        "colab_type": "text"
      },
      "source": [
        "## ** Restart the Colab kernel after every pip install**\n",
        "\n",
        "Runtime -> Restart Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zj5DFniQ11y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPORTANT: RESTART THE COLAB KERNEL if you've just run !pip install\n",
        "\n",
        "# Test the installation\n",
        "import gym_fryshome\n",
        "\n",
        "gym_fryshome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2iBf7ZeJgeT",
        "colab_type": "text"
      },
      "source": [
        "## Random Walk Agent\n",
        " \n",
        "The simplest way is to have Bender randomly walk around the house.\n",
        "* We will run 20 episodes, where each episode is a maximum of 100 steps\n",
        "* Refer to http://gym.openai.com/docs/\n",
        "\n",
        "This doesn't actually learn anything, but is a good baseline for any RL agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycXafXEJ-tR",
        "colab_type": "code",
        "outputId": "2292eb69-efec-40f0-c47d-21804dfc4e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make('gym_fryshome:fryshome-v0')\n",
        "\n",
        "for episode in range(20):\n",
        "  observation = env.reset()\n",
        "\n",
        "  for t in range(100):\n",
        "    env.render()\n",
        "    \n",
        "    # take a random action\n",
        "    action = env.action_space_sample() # random actions\n",
        "\n",
        "    # step the environment using the selected action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      print(f'Episode finished after {t+1} timesteps\\n')\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 19 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 16 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 4 timesteps\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blqh8_sZbXEW",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning Agent\n",
        "\n",
        "Let's implement the basic Q-Learning formula (without Temporal Differencing):\n",
        "\n",
        "`Q(state, action) = R(state, action) + gamma * max[Q(next_state, all actions)]`\n",
        "\n",
        "Our Q-Learning agent will store the Q-values as we go along. This can be thought of as Benders \"brain.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5D-vv4_cWox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_aCvSQRd1WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkaJhf12dham",
        "colab_type": "text"
      },
      "source": [
        "## More Advanced Agents\n",
        "\n",
        "You can install baselines, which contain implementation of more sophisticated agents.\n",
        "\n",
        "https://github.com/openai/baselines"
      ]
    }
  ]
}