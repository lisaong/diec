{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "path_finding_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOzj90aBFFkHMz3k15vbtKN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/diec/blob/rl_path_finding/day4/rl/path_finding_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxsIdZTi60PM",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning Path-Finding Demo\n",
        "\n",
        "This demonstrates how to:\n",
        "- Use OpenAI gym to create a custom environment\n",
        "- Compare different Q-learning algorithms for Reinforcement Learning\n",
        "\n",
        "Inspired by: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
        "\n",
        "## Problem Setup\n",
        "\n",
        "Bender is lost in Fry's house! Help Bender find Fry (who is in Room 5 waiting with a can of beer).\n",
        "\n",
        "![intro](https://github.com/lisaong/diec/raw/rl_path_finding/day4/rl/path_finding_intro.png)\n",
        "\n",
        "## OpenAI Gym\n",
        "\n",
        "[OpenAI Gym](https://gym.openai.com/) is an open-source Python toolkit for developing RL algorithms.\n",
        "\n",
        "We will use OpenAI gym to re-create Fry's house, then run some reinforcement learning to find the path.\n",
        "\n",
        "https://github.com/openai/gym/blob/master/docs/creating-environments.md\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvDQ9PwW6SFZ",
        "colab_type": "code",
        "outputId": "f87f8df8-d0eb-4c42-d516-9ff3a2fb9d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# gym is already built into Colab\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "gym.__version__"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.15.6'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GBlQwqV_dSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# A Simple Path Finding OpenAI Gym Environment\n",
        "#\n",
        "# Inspired by: http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
        "#\n",
        "\n",
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class FrysHomeEnv(gym.Env):\n",
        "  \"\"\"Custom Environment describing Fry's home  \n",
        "  \n",
        "  For details on the gym.Env class:\n",
        "  https://github.com/openai/gym/blob/master/gym/core.py\n",
        "  \"\"\"\n",
        "\n",
        "  # render to the current display or terminal\n",
        "  metadata = {'render.modes': ['human']}\n",
        "\n",
        "  def __init__(self, rewards):\n",
        "    super(FrysHomeEnv, self).__init__()\n",
        "\n",
        "    self.rewards = rewards\n",
        "    self.num_rooms = self.rewards.shape[0]\n",
        "\n",
        "    # Action space describes all possible actions that can be taken\n",
        "    # here, we can select 1 out of 6 rooms\n",
        "    self.action_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Observation space describes the valid observations\n",
        "    # since we are moving between rooms, we can be in 1 of 6 rooms\n",
        "    self.observation_space = spaces.Discrete(self.num_rooms)\n",
        "\n",
        "    # Rewards range describes the min and max possible rewards\n",
        "    self.reward_range = (self.rewards.min(), self.rewards.max())\n",
        "\n",
        "    # Room 5 is our goal\n",
        "    self.goal = 5\n",
        "\n",
        "    # Initialise our state\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Reset the environment to an initial state\"\"\"\n",
        "\n",
        "    # Randomly initialise the state\n",
        "    self.state = random.randint(0, self.num_rooms-1)\n",
        "\n",
        "    # Return the observation (same as the state in our case)\n",
        "    obs = self.state\n",
        "    return obs\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"Execute one step within the environment\"\"\"\n",
        "\n",
        "    # take the selected action\n",
        "    prev_state = self.state\n",
        "    self.state = action\n",
        "\n",
        "    # calculate the reward\n",
        "    reward = self.rewards[prev_state][action]\n",
        "\n",
        "    # check if we've reached our goal\n",
        "    done = (prev_state == self.goal or self.state == self.goal)\n",
        "\n",
        "    # get the next observation\n",
        "    obs = self.state\n",
        "\n",
        "    return obs, reward, done, {}\n",
        "\n",
        "  def render(self, mode='human', close=True):\n",
        "    \"\"\"Print state of the current environment\"\"\"\n",
        "    print(f'Current room: {self.state}, Reached goal: {self.state == self.goal}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyjONcEzEzTj",
        "colab_type": "code",
        "outputId": "e93380b5-d60a-4581-b91a-49634e62eb9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Unit testing\n",
        "\n",
        "# Initialise the rewards matrix for the rooms in the house\n",
        "# Where:\n",
        "#  state: current room, action: next room\n",
        "#  dimensions (row=state, col=actions)\n",
        "#  A value of -1 means there is no adjacent path from room_i to room_j\n",
        "#  (for example, room_0 to room_0 has, room_0 to room_5)\n",
        "R = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "              [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "              [-1, -1, -1,  0, -1, -1], # etc\n",
        "              [-1,  0,  0, -1,  0, -1],\n",
        "              [ 0, -1, -1,  0, -1,  0],\n",
        "              [-1, 100, -1, -1, 100, 100]])\n",
        "\n",
        "myenv = FrysHomeEnv(rewards=R)\n",
        "\n",
        "for i in range(0, 6):\n",
        "  print(myenv.step(i))\n",
        "  myenv.render()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, -1, True, {})\n",
            "Current room: 0, Reached goal: False\n",
            "(1, -1, False, {})\n",
            "Current room: 1, Reached goal: False\n",
            "(2, -1, False, {})\n",
            "Current room: 2, Reached goal: False\n",
            "(3, 0, False, {})\n",
            "Current room: 3, Reached goal: False\n",
            "(4, 0, False, {})\n",
            "Current room: 4, Reached goal: False\n",
            "(5, 0, True, {})\n",
            "Current room: 5, Reached goal: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ8RA_ilKtkX",
        "colab_type": "text"
      },
      "source": [
        "## Package custom environment as module\n",
        "\n",
        "OpenAI gym requires all environments to be packaged as Python modules.\n",
        "\n",
        "The code above has been packaged here:\n",
        "https://github.com/lisaong/diec/blob/master/day4/rl/gym-fryshome\n",
        "\n",
        "The module follows this convention:\n",
        "https://github.com/openai/gym/blob/master/docs/creating-environments.md"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBhgYzDkN46S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ded5eeee-80c1-4346-8b85-059aa457379e"
      },
      "source": [
        "!git clone -b rl_path_finding https://github.com/lisaong/diec.git\n",
        "%cd diec/day4/rl/gym-fryshome\n",
        "!git pull\n",
        "!pip install --verbose -e  ."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'diec' already exists and is not an empty directory.\n",
            "/content/diec/day4/rl/gym-fryshome\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 17 (delta 11), reused 12 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (17/17), done.\n",
            "From https://github.com/lisaong/diec\n",
            "   f52e36d..b8d2558  rl_path_finding -> origin/rl_path_finding\n",
            "Updating f52e36d..b8d2558\n",
            "Fast-forward\n",
            " .../gym_fryshome/envs/frys_home_env.py             |  16 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " day4/rl/path_finding_demo.ipynb                    | 254 \u001b[32m++++++++++++\u001b[m\u001b[31m---------\u001b[m\n",
            " 2 files changed, 144 insertions(+), 126 deletions(-)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-39sk297y\n",
            "Created temporary directory: /tmp/pip-req-tracker-w83an5vv\n",
            "Created requirements tracker '/tmp/pip-req-tracker-w83an5vv'\n",
            "Created temporary directory: /tmp/pip-install-883a0bjx\n",
            "Obtaining file:///content/diec/day4/rl/gym-fryshome\n",
            "  Added file:///content/diec/day4/rl/gym-fryshome to build tracker '/tmp/pip-req-tracker-w83an5vv'\n",
            "    Running setup.py (path:/content/diec/day4/rl/gym-fryshome/setup.py) egg_info for package from file:///content/diec/day4/rl/gym-fryshome\n",
            "    Running command python setup.py egg_info\n",
            "    running egg_info\n",
            "    writing gym_fryshome.egg-info/PKG-INFO\n",
            "    writing dependency_links to gym_fryshome.egg-info/dependency_links.txt\n",
            "    writing requirements to gym_fryshome.egg-info/requires.txt\n",
            "    writing top-level names to gym_fryshome.egg-info/top_level.txt\n",
            "    writing manifest file 'gym_fryshome.egg-info/SOURCES.txt'\n",
            "  Source in /content/diec/day4/rl/gym-fryshome has version 0.0.1, which satisfies requirement gym-fryshome==0.0.1 from file:///content/diec/day4/rl/gym-fryshome\n",
            "  Removed gym-fryshome==0.0.1 from file:///content/diec/day4/rl/gym-fryshome from build tracker '/tmp/pip-req-tracker-w83an5vv'\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-fryshome==0.0.1) (0.15.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gym-fryshome==0.0.1) (1.17.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.4.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.12.0)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-fryshome==0.0.1) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-fryshome==0.0.1) (0.16.0)\n",
            "Installing collected packages: gym-fryshome\n",
            "  Found existing installation: gym-fryshome 0.0.1\n",
            "    Not sure how to uninstall: gym-fryshome 0.0.1 - Check: /content/diec/day4/rl/gym-fryshome\n",
            "    Can't uninstall 'gym-fryshome'. No files were found to uninstall.\n",
            "  Running setup.py develop for gym-fryshome\n",
            "    Running command /usr/bin/python3 -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/content/diec/day4/rl/gym-fryshome/setup.py'\"'\"'; __file__='\"'\"'/content/diec/day4/rl/gym-fryshome/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' develop --no-deps\n",
            "    running develop\n",
            "    running egg_info\n",
            "    writing gym_fryshome.egg-info/PKG-INFO\n",
            "    writing dependency_links to gym_fryshome.egg-info/dependency_links.txt\n",
            "    writing requirements to gym_fryshome.egg-info/requires.txt\n",
            "    writing top-level names to gym_fryshome.egg-info/top_level.txt\n",
            "    writing manifest file 'gym_fryshome.egg-info/SOURCES.txt'\n",
            "    running build_ext\n",
            "    Creating /usr/local/lib/python3.6/dist-packages/gym-fryshome.egg-link (link to .)\n",
            "    gym-fryshome 0.0.1 is already the active version in easy-install.pth\n",
            "\n",
            "    Installed /content/diec/day4/rl/gym-fryshome\n",
            "Successfully installed gym-fryshome\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-w83an5vv'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7ITTKEVV_O8",
        "colab_type": "text"
      },
      "source": [
        "## ** Restart the Colab kernel after every pip install**\n",
        "\n",
        "Runtime -> Restart Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Zj5DFniQ11y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d31f79c5-c7d1-46e8-df36-80827bb79a4a"
      },
      "source": [
        "# IMPORTANT: RESTART THE COLAB KERNEL if you've just run !pip install\n",
        "\n",
        "# Test the installation\n",
        "import gym_fryshome\n",
        "\n",
        "gym_fryshome"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'gym_fryshome' from '/content/diec/day4/rl/gym-fryshome/gym_fryshome/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2iBf7ZeJgeT",
        "colab_type": "text"
      },
      "source": [
        "## Random Walk Agent\n",
        " \n",
        "The simplest way is to have Bender randomly walk around the house.\n",
        "* We will run 20 episodes, where each episode is a maximum of 100 steps\n",
        "* Refer to http://gym.openai.com/docs/\n",
        "\n",
        "This doesn't actually learn anything, but is a good baseline for any RL agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLpFOINPf9Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomAgent:\n",
        "    \"\"\"The world's simplest agent!\n",
        "      https://github.com/openai/gym/blob/master/examples/agents/random_agent.py\n",
        "    \"\"\"\n",
        "    def __init__(self, action_space):\n",
        "        self.action_space = action_space\n",
        "\n",
        "    def act(self, observation, reward, done):\n",
        "        return self.action_space.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XycXafXEJ-tR",
        "colab_type": "code",
        "outputId": "485e3a71-75f5-4da7-fb3c-3e0d784ba0b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Constants\n",
        "EPISODE_COUNT = 20\n",
        "STEPS_PER_EPISODE = 100\n",
        "R = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "              [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "              [-1, -1, -1,  0, -1, -1], # etc\n",
        "              [-1,  0,  0, -1,  0, -1],\n",
        "              [ 0, -1, -1,  0, -1,  0],\n",
        "              [-1, 100, -1, -1, 100, 100]])\n",
        "\n",
        "# Global state\n",
        "done = False\n",
        "reward = 0\n",
        "\n",
        "# Create our environment (Fry's home), and our agent\n",
        "env = gym.make('gym_fryshome:fryshome-v0', rewards=R)\n",
        "bender_agent = RandomAgent(env.action_space)\n",
        "\n",
        "for episode in range(EPISODE_COUNT):\n",
        "  observation = env.reset()\n",
        "\n",
        "  for t in range(STEPS_PER_EPISODE):\n",
        "    env.render()\n",
        "    \n",
        "    # take a random action\n",
        "    action = bender_agent.act(observation, reward, done)\n",
        "\n",
        "    # step the environment using the selected action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      print(f'Episode finished after {t+1} timesteps\\n')\n",
        "      done = False # reset for next episode\n",
        "      break\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 9 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 13 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 0, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 4, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Current room: 2, Reached goal: False\n",
            "Current room: 3, Reached goal: False\n",
            "Current room: 1, Reached goal: False\n",
            "Episode finished after 8 timesteps\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blqh8_sZbXEW",
        "colab_type": "text"
      },
      "source": [
        "## Q-Learning Agent\n",
        "\n",
        "Let's implement the basic Q-Learning formula (without Temporal Differencing):\n",
        "\n",
        "`Q(state, action) = R(state, action) + gamma * max[Q(next_state, all actions)]`\n",
        "\n",
        "Our Q-Learning agent will store the Q-values as we go along. This can be thought of as Benders \"brain\"!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5D-vv4_cWox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "class QLearningAgent:\n",
        "  \"\"\"Basic Q-Learning Agent\"\"\"\n",
        "  def __init__(self, rewards, gamma=0.8):\n",
        "    \"\"\"rewards: the rewards matrix\n",
        "    gamma: the discount factor in considering future rewards\n",
        "    \"\"\"\n",
        "    self.rewards = rewards\n",
        "\n",
        "    self.action_space = spaces.Discrete(rewards.shape[0])\n",
        "    self.action = self.action_space.sample()\n",
        "\n",
        "    self.actions = np.arange(rewards.shape[0])\n",
        "\n",
        "    self.gamma = gamma\n",
        "\n",
        "    # Initialise the Q-matrix to zeros:\n",
        "    # dimensions (row=state, cols=actions)\n",
        "    self.Q = np.zeros(rewards.shape)\n",
        "\n",
        "  def _get_valid_actions(self, observation):\n",
        "    return self.actions[self.rewards[observation] != -1]\n",
        "\n",
        "  def act(self, observation, reward, done):\n",
        "    \"\"\"Update the Q-matrix, then take an action\n",
        "    observation: current state\n",
        "    reward: reward from the previous action\n",
        "    done: whether the episode is completed\n",
        "    \"\"\"\n",
        "    if done:\n",
        "      return self.action # no change, we are done\n",
        "\n",
        "    # randomly select the next action (and next observation)\n",
        "    valid_actions = self._get_valid_actions(observation)\n",
        "    self.action = np.random.choice(valid_actions)\n",
        "    next_observation = self.action\n",
        "\n",
        "    # find the maximum reward for all actions given the next observation\n",
        "    all_actions = self._get_valid_actions(next_observation)\n",
        "    future_rewards = self.rewards[next_observation][all_actions]\n",
        "\n",
        "    print(f'Next action/observation: {next_observation}, all actions: \\\n",
        "{all_actions}, all future rewards: {future_rewards}, max future reward: \\\n",
        "{future_rewards.max()}')\n",
        "\n",
        "    # update the Q matrix\n",
        "    self.Q[observation][self.action] = self.rewards[observation][self.action] + \\\n",
        "      self.gamma * future_rewards.max()\n",
        "    print(f'Q-values:\\n{self.Q}')\n",
        "\n",
        "    return self.action"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_aCvSQRd1WA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "24e71d14-4fe3-4ba9-9609-97f703514864"
      },
      "source": [
        "# Unit Test\n",
        "R = np.array([[-1, -1, -1, -1,  0, -1], # action 0\n",
        "              [-1, -1, -1,  0, -1, 0],  # action 1\n",
        "              [-1, -1, -1,  0, -1, -1], # etc\n",
        "              [-1,  0,  0, -1,  0, -1],\n",
        "              [ 0, -1, -1,  0, -1,  0],\n",
        "              [-1, 100, -1, -1, 100, 100]])\n",
        "\n",
        "test_agent = QLearningAgent(rewards=R)\n",
        "test_agent.act(1, 0, False)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0. 80.]\n",
            " [ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LDllxIHsHkZ",
        "colab_type": "text"
      },
      "source": [
        "Put our agent to work. Hopefully Bender is now smarter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHBprCxpsHBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b744cbb-676f-414c-9023-cdf8c9ba34b6"
      },
      "source": [
        "# Global state\n",
        "EPISODE_COUNT = 1000\n",
        "STEPS_PER_EPISODE = 20\n",
        "done = False\n",
        "reward = 0\n",
        "\n",
        "# Track how many timesteps it took to finish\n",
        "timesteps_history = []\n",
        "\n",
        "# Create our environment (Fry's home), and our new agent\n",
        "env = gym.make('gym_fryshome:fryshome-v0', rewards=R)\n",
        "bender_v2 = QLearningAgent(rewards=R)\n",
        "\n",
        "for episode in range(EPISODE_COUNT):\n",
        "  observation = env.reset()\n",
        "\n",
        "  for t in range(STEPS_PER_EPISODE):\n",
        "    env.render()\n",
        "    \n",
        "    # take the next action\n",
        "    action = bender_v2.act(observation, reward, done)\n",
        "\n",
        "    # step the environment using the selected action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "\n",
        "    if done:\n",
        "      print(f'Episode finished after {t+1} timesteps\\n')\n",
        "      timesteps_history.append(t)\n",
        "      done = False # reset for next episode\n",
        "      break\n",
        "    \n",
        "env.close()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 13 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 19 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 16 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 9 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 14 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 9 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 15 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 9 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 13 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 4 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 14 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 11 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 17 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 13 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 13 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 12 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 10 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 6 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 17 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 5 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 4 timesteps\n",
            "\n",
            "Current room: 5, Reached goal: True\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 1 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 4 timesteps\n",
            "\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 0, all actions: [4], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 3 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 12 timesteps\n",
            "\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 7 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 2, all actions: [3], all future rewards: [0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 2, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 8 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 4 timesteps\n",
            "\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 2 timesteps\n",
            "\n",
            "Current room: 0, Reached goal: False\n",
            "Next action/observation: 4, all actions: [0 3 5], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 4, Reached goal: False\n",
            "Next action/observation: 3, all actions: [1 2 4], all future rewards: [0 0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 3, Reached goal: False\n",
            "Next action/observation: 1, all actions: [3 5], all future rewards: [0 0], max future reward: 0\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Current room: 1, Reached goal: False\n",
            "Next action/observation: 5, all actions: [1 4 5], all future rewards: [100 100 100], max future reward: 100\n",
            "Q-values:\n",
            "[[  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.   0.]\n",
            " [  0.   0.   0.   0.   0.  80.]\n",
            " [  0. 100.   0.   0. 100. 180.]]\n",
            "Episode finished after 4 timesteps\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smpLf_eVuLNu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "59bd348e-ffdd-473c-c0b7-1b2fad9934f0"
      },
      "source": [
        "# plot the history\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(np.arange(len(timesteps_history)), timesteps_history)\n",
        "plt.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkaJhf12dham",
        "colab_type": "text"
      },
      "source": [
        "## More Advanced Agents\n",
        "\n",
        "You can install baselines, which contain implementation of more sophisticated agents.\n",
        "\n",
        "https://github.com/openai/baselines"
      ]
    }
  ]
}